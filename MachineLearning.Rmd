---
title: "Prediciton Assignment - Practical Machine Learning"
author: "Oscar Reyes"
date: "July 25, 2015"
output:
  html_document:
    toc: yes
    number_sections: true
    fig_height: 7
    fig_width: 8
    highlight: zenburn
---

#Overview
Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement â€“ a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, our goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: <http://groupware.les.inf.puc-rio.br/har> (see the section on the Weight Lifting Exercise Dataset).  

#Data
|Type|Source|
| :--- | :--- |
|Training|<https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv>|
|Test |<https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv>|
|**Source**|<http://groupware.les.inf.puc-rio.br/har>|

#Objective
Predict the manner in which they did the exercise.  This is the **"classe"** variable in the training set.

#Project
##Libraries
```{r, echo=TRUE, warning=FALSE}
# Loading Libraries
library (caret)
library (ggplot2)
library (knitr)
library (lattice)
library (randomForest)
library (rattle)
library (RColorBrewer)
library (rpart)
library (rpart.plot)
set.seed(1234)
```

##Loading Data
```{r, echo=TRUE, warning=FALSE}
##Loading Data
home = setwd(Sys.getenv("HOME"))

# Source Directory
swdMain <- "/Users/oreyesc"
swdSubDir <- "MachineLearningProject"
getswd <- getwd()

# URLs
trainingURL <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
testingURL <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"

# File names:
trainingfile <- "pml-training.csv"
testingfile <- "pml-testing.csv"


if (getswd != swdMain){
        if (!file.exists(swdMain)){
                dir.create(file.path(swdMain,
                                     swdSubDir),
                           showWarnings = FALSE)
        }

} else if (!file.exists(swdSubDir)){
        dir.create(file.path(swdSubDir),
                   showWarnings = FALSE)
}
setwd (file.path(swdMain, swdSubDir))

# Downloading the information:
if (!file.exists(trainingfile)){
        download.file(url = trainingURL,
                      destfile = trainingfile,
                      method = "curl")
}
if (!file.exists(testingfile)){
        download.file(url = testingURL,
                      destfile = testingfile,
                      method = "curl")
}
# Loading information from files:
trainingData <- read.csv(trainingfile,
                         row.names = 1,
                         header = TRUE,
                         sep = ",",
                         na.strings = c("NA",
                                        "#DIV/0!",
                                        ""))

testingData <- read.csv(testingfile,
                        row.names = 1,
                        header = TRUE,
                        sep = ",",
                        na.strings = c("NA",
                                       "#DIV/0!",
                                       ""))
trainingData <- trainingData[, -1]
```

##Preprocessing Data
##Creating Testing and Training Datasets

| Dataset          | Percentage |
| :--------------- | :--------- |
| **trainingData** | **100%**   |
| training         | 60%        |
| testingTraining  | 40%        |

```{r, echo=TRUE, warning=FALSE}
#Datasets division
##Create data partitions from training dataset: training 60% and testing 40%
trainingDivision = createDataPartition(trainingData$classe,
                                       p = 0.60,
                                       list = FALSE)
training <- trainingData[trainingDivision, ]
testingTraining <- trainingData[-trainingDivision, ]
dim(training); dim(testingTraining)
```

##Cleaning Data
1. Identify how may columns are with less than 60% of data:
```{r, echo=TRUE, warning=FALSE}
#Exploration & Cleaning
#Remove all information with less than 60% of fields filled
#Quantity of columns that have less than 60% of fields filled
sum((colSums(!is.na(training[,
                             -ncol(training)])) < 0.6 * nrow(training)))
```
2. Select the columns with more than 60% of information:
```{r, echo=TRUE, warning=FALSE}
#Removing columns with poor data (less than 60%)
filledColumns <- c((colSums(!is.na(training[,
                                            -ncol(training)])) >= 0.6 * nrow(training)))
training <- training[,
                     filledColumns]
testingTraining <- testingTraining[, filledColumns]
dim(training)
dim(testingTraining)
```

##Model
###Random Forest
`Random Forest` allow to stimate the `test set error` into the execution, so `cross-validation` or a `test set` are not required to obtain an unbiased estimete of `test set error`.
```{r, echo=TRUE, warning=FALSE}
#Random Forest Model
# Cross validation or separate test set is not required in random forest to obtain an unbiased estimate of the test set error.
# The test set error is evaluated by the aplication, on the execution.

randomForestModel <- randomForest(classe ~.,
                                  method = "rf",
                                  data = training,
                                  importance = TRUE)
print (randomForestModel)
```

####Evaluationg Random Forest Model - Random Forest
Verification of variable importance:
```{r, echo=TRUE, warning=FALSE}
#Evaluating the Model
# Variable importance measures:
importance(randomForestModel)
````

####Confusion Matrix - Random Forest
```{r, echo=TRUE, warning=FALSE}
#Confusion Matrix
predictTraining <- predict(randomForestModel,
                           newdata = testingTraining[, -ncol(testingTraining)])
confusionMatrix(predictTraining,
                testingTraining$classe)
```

####Validation - Random Forest
```{r, echo=TRUE, warning=FALSE}
#Accuracy validation
accuracyRandomForest <- c(as.numeric(predictTraining == testingTraining$classe))
accuracyRandomForest <- sum(accuracyRandomForest) * 100 / nrow(testingTraining)
accuracyRandomForest

outOfSampleError <- 1 - accuracyRandomForest
```

| Model           | Accuracy     | Out Of Sample Error |
| :-------------- | :----------- | :------------------ |
| `Random Forest` | **99.8598%** | **0.1911802%**      |

```{r, echo=TRUE, warning=FALSE}
importantVariable <- varImp(randomForestModel)$importance
varImpPlot(randomForestModel,
           sort = TRUE,
           type = 1,
           pch = 19,
           col = 1,
           cex = 1,
           main = "Predictors - Level of Importance")
```

###Decision Tree
```{r, echo=TRUE, warning=FALSE}
#Decision Trees
decisionTree <- rpart(classe ~.,
                      data = training,
                      method = "class")
fancyRpartPlot(decisionTree)
predictionDecisionTree <- predict(decisionTree,
                                   testingTraining,
                                   type = "class")

confusionMatrix(predictionDecisionTree,
                testingTraining$classe)
```

####Validation - Decisiont Tree
```{r, echo=TRUE, warning=FALSE}
accuracyDecisionTree <- c(as.numeric(predictionDecisionTree == testingTraining$classe))
accuracyDecisionTree <- sum(accuracyDecisionTree) * 100 / nrow(testingTraining)
accuracyDecisionTree

outOfSampleErrorDT <- 100 - accuracyDecisionTree
outOfSampleErrorDT
```

| Model           | Accuracy   | Out Of Sample Error |
| :-------------- | :--------- | :------------------ |
| `Decision Tree` | **87.40%** | **12.5924%**        |

###Comparisson of Models
| Model           | Accuracy     | Out Of Sample Error |
| :-------------- | :----------- | :------------------ |
| `Random Forest` | **99.8598%** | **0.1911802%**      |
| `Decision Tree` | **87.74%**   | **12.5924%**        |


`Random Forest Model` has a better accuracy level.

##Predictions
Working with the `Random Model`, because has a better accuracy level:
```{r, echo=TRUE, warning=FALSE}
#Testing the Model
testingD <- testingData[, -1]
testingD <- testingD[,
                     filledColumns]
testingD <- testingD[,
                     -ncol(testingD)]
testing <- rbind(training[100, -58],
                 testingD)
row.names(testing) <- c(100, 1:20)

predictions <- predict(randomForestModel,
                       newdata = testing[-1, ])
```
#Answers
The following function creates the files. If have a character vector with 20 predictions in order for the 20 problems. 
```{r, echo=TRUE, warning=FALSE}
pml_write_files = function(x){
        n = length(x)
        for(i in 1:n){
                filename = paste0("./answers/problem_id_",i,".txt")
                write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
        }
}
pml_write_files(predictions)
print (predictions)
````

**Note**: The `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.   

**GitHub Repo**: <https://github.com/oreyesc/Project---Machine-Learning>   
**RPubs**: <http://rpubs.com/oreyesc/95547>   

